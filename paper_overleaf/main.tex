\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[super]{nth}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage[
singlelinecheck=false
]{caption}
\usepackage{amsmath}
\usepackage{bbm}
\title{Method to infer relatedness using low coverage ancient DNA}
\author{Divya Ratan Popli, Benjamin M. Peter}
\date{5 August 2021}
\linenumbers

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\newcommand{\BZ}{\mathbf{Z}}
\newcommand{\BD}{\mathbf{D}}
\newcommand{\BH}{\mathbf{H}}

\begin{document}


\maketitle

\begin{abstract}
\noindent Knowledge of familial relationships is an important part of several research fields. However, estimating relatedness from ancient DNA can be be difficult since DNA sequences extracted from ancient bones may have low coverage, ascertainment bias (if the sequences have been captured with DNA probes), and contamination from modern humans. In addition, the population of interest may have long runs of homozygosity (ROH) due to recent inbreeding or/and small population size. This can affect the relatedness estimates. Here, we present a Hidden Markov Model (HMM) to estimate identity by descent (IBD) fragments and pairwise relatedness upto 3rd degree, while additionally differentiating between siblings and parent-child. We have developed another HMM to estimate ROH proportions, that can be utilized by the relatedness estimation model to output better estimates. 
\end{abstract}

\section{Introduction}

\subsection{Why study relatedness?}
Identifying related individuals is a common task in applications of genetics. Relatedness is of direct interest in e.g. DNA forensics, where familial search can aid in solving criminal cases, and to identify missing persons following a disaster \cite{murphy_law_2018,ram_genealogy_2018}. Genetic paternity tests have an important application in resolving family relation, e.g. in establishing relationship between a person applying for immigration and the claimed relatives \cite{egeland_beyond_2000}. It is also an essential preprocessing step in population genetics and association studies, where samples are typically assumed to be independent random draws from the population.

In ancient DNA, relatedness can be used to identify bones and teeth belonging to the same individual, and can provide an  understanding of an ancient society's organization and hierarchy, social structures, and cultural aspects ~\cite{baca_ancient_2012,mittnik_kinship-based_2019,sikora_ancient_2017}. For animal and plant breeders and conservation biologists, reconstructing pedigrees and finding related individuals in the mating animals is important to ensure diversity. ~\cite{habier_impact_2007,oliehoek_estimating_2006,kardos_measuring_2015} 

\subsection{Simple way to get relatedness, methods that do it with a lot of data}
Commonly, pairs of relateded individuals are  identified by looking for parts of the genome that are identical by descent (IBD), ie. inherited from a recent common ancestor. Due to the laws of Mendelian segregation, a parent, for example, will share exactly one set of chromosome IBD with its offspring, with the other set of chromosomes in the offspring coming from the other parent. A grandparent will, on average share a quarter of its genome with a grand-child, because recombination broke up the chromosomes.

However, it is not possible to directly observe IBD segments, and so most methods infer them by first identifying segments of the genome that are Identical by State (IBS) and using population allele frequencies to calculate the probability of IBD given IBS \cite{vai_kinship_2020}.  One exception is SNPduo, a software that uses IBS direclty to visualize and analyze pairwise relatedness. \cite{roberson_visualization_2009}. Most methods, however, use IBD as an intermediary in inferring relatedness \cite{boehnke_accurate_1997,lynch_estimation_1999,mcpeek_statistical_2000}. For a non-inbred population, there are three IBD states possible: either no chromosomes are shared, only one chromosome is shared, or both chromosomes are shared. The genome-wide proportions of these states (usually referred to as $k_0$, $k_1$, $k_2$, so that $k_0+k_1+k_2=1$) can be used to infer the degree and nature of relatedness for a pair of individuals. For example, a pair of siblings are expected to have all three possible IBD states with probabilities (0.25,0.5,0.25) as shown in fig 1. These IBD probabilities can directly be used to categorize a pair of individuals as related by a certain degree and nature of relatedness as shown in table1. One can also use these probabilities to estimate the coefficient of relatedness as $r= k_1/2 + k_2$. The coefficient of relatedness (r) is defined as the proportion of genome IBD for a pair of individuals.
There are numerous methods that utilize population allele frequencies, phase information, recombination maps, or genotype calls to estimate the relatedness coefficient \cite{huff_maximum-likelihood_2011,li_relationship_2014,li_accurate_2014,thornton_estimating_2012}. PLINK, a popular tool to infer relatedness coefficient in  diploid genotype data, estimates IBD probabilities from average of observed IBS, and using the allele frequencies at each SNP in samples \cite{purcell_plink_2007}. KING is another widely used software that allows relatedness inference with genotype data in homogeneous populations using population allele frequencies (KING homo), but can also work with structured populations with no good population allele frequencies using heterozygosity estimates from each sample (KING robust) \cite{manichaikul_robust_2010}. . 

\begin{table}
\caption{\label{tab:Table 1}IBD sharing probabilities for different relations}
\begin{tabular}{|c|c|c|c|}
    \hline
    Relatedness & $k_0$ & $k_1$ & $k_2$\\
    \hline
    Unrelated & 1 & 0 & 0\\
    \hline
    3rd Degree & 0.75 & 0.25 & 0\\
    \hline
    2nd Degree & 0.5 & 0.5 & 0\\
    \hline
    Siblings & 0.25 & 0.5 & 0.25\\
    \hline
    Parent-Child & 0 & 1 & 0\\
    \hline
    Identical/twins & 0 & 0 & 1\\
    \hline
\end{tabular}

\end{table}

\subsection{Problems with ancient data, and methods that address these issues}
One major issue with applying above mentioned methods to ancient DNA is that, frequently, the amount of endogenous DNA is very low, making it very difficult to get genotype calls.
Several methods solve this problem by using genotype likelihoods \cite{lipatov_maximum_2015,korneliussen_ngsrelate_2015}. These methods account for the uncertainity in genotype calls by summing over all possible genotypes, weighted by the genotype likelihoods. However, these approaches typically still require at least 2x coverage, since genotype likelihoods may not be very informative at lower coverages. Apart from the low amount of endogenous DNA sequences present, ancient DNA analyses often have additional problems like contamination from modern populations \cite{peyregne_authentict_2020}, absence of a good reference panel, and ascertainment bias in case of captured sequences. In cases where allele frequency or a reference panel for the target population is not available, it is possible to use allele frequency from a modern population from the same geographic location, or to try using population allele frequencies from multiple potentially close populations. However, incorrect assumptions may cause the individuals in the target population to look more similar or different to each other \cite{amorim_understanding_2018}. Several methods have been proposed to estimate relatedness without a reference panel, but many of these require either $>4x$ coverage to get a good estimate of genotype likelihoods\cite{waples_allele_2019}, or a large sample size to get an estimate of population allele frequencies from the samples \cite{theunert_joint_2017}. In this study, we show that contamination from modern humans can cause some of the individuals in ancient populations to look more unrelated than they are. This happens because of comparison of endogenous reads to contaminating reads which would give higher differences than comparison of endogenous reads from individuals from ancient population. Hence, contamination can reduce the power to detect related pairs, especially when the target population is quite diverged from contaminating population(s). In addition, the target populations may have long runs of homozygosity (ROH) due to a small population size, or recent inbreeding. Long ROH causes related individuals to seem genetically closer to each other, while does not affect the genetic distance between unrelated individuals. This effect may cause an increase in false positives. In many cases, ancient DNA is captured with a SNP array, and the SNPs are ascertained to variable sites determined from certain individuals. Relatedness methods based on the fraction of sites in different IBS states are highly sensitive to ascertainment bias. KING robust, for example, differentiates between different relatedness by plotting KING‚Äêrobust kinship against fraction of sites in IBS0 \cite{manichaikul_robust_2010}. Another method from Rosenberg,2006 \cite{rosenberg_standardized_2006} uses the fraction of sites in all three IBS states to classify related individuals in the HGDP-CEPH Human Genome Diversity Cell Line Pane. These methods may give different results when applied to data with different ascertainment schemes \cite{waples_allele_2019}. 

READ is a software that does not require population allele frequencies or a reference panel, and can work with around 0.5 x coverage in presence of ascertainment bias. The software solves the problem of unavailability of genotype calls by randomly sampling alleles from each individual. A string of these alleles at each position (called pseudohaploids) are then compared to other individuals to calculate pairwise genetic distances, which in turn are used to infer relatedness. The limitations are that the software can identify upto only second degree relatives, without differentiating between siblings and parent-child, which may be crucial when making pedigrees. 

\subsection{How our method works}
Here, we present KIn (Kinship Inference), a Hidden Markov Model (HMM) to estimate relatedness from low-coverage ancient DNA data. KIn can detect up to \nth{3} degree relatives, and differentiates between siblings and parent-child. KIn is also able to incorporate ROH, contamination and ascertainment bias. We validate the performance of KIn using simulations and show that we are able to infer relatedness in an archaic human data set.



\section{Methods}


The core of  KIn, is a HMM that aims to infer relatedness and IBD sharing between a pair of low-coverage individuals, optionally taking ROH tracts in each sample, and contamination estimates into account. If the locations of ROH tracts are unknown, we provide another HMM to estimate ROH in the first step for a sample of sufficient coverage ($\geq 0.5x$). In a second step, the relatedness-HMM uses the contamination-corrected differences between a pair of individuals, and the positions of ROH tracts, to infer relatedness.  Our method is available on \url{https://github.com/DivyaratanPopli/KIn_snakemake} along with a \textt{snakemake} \cite{koster_snakemakescalable_2012} pipeline to generate the input files for the models directly from bam files. 


\subsection{Relatedness Model overview} 
To estimate the relatedness of two individuals, we subdivide the genome into $L$ large windows $w$ (typically 10Mb). The inputs of our algorithm are i) the number of sites $N_w$ for which both samples have at least one read available, ii) the number of pairwise differences $D_w$ at these sites, and iii) position of ROH, by default obtained from ROH-HMM described in a following section. 

Throughout, we will use bold-face notation to refer to the vector (or matrix) collecting  all the terms, e.g. $\BD = (D_1, D_2, \dots D_L)$. 

The model then uses this data for a given pair of individuals to classify each window into three hidden states $Z_w$ reflecting zero, one or two chromosomes shared IBD. Likewise, we classify each window into one of three possible ROH state $H_w$, reflecting, neither, one or both indivuals being in a run-of-homozygosity at this genomic location.

Ideally, we would like to compute the likelihood $P(\BD | \BZ, \BH, \theta$), where  $\theta$ is a vector of parameters (initial transition probability $\pi$, transition matrix $A$ and $N_w$). 

However, calculating this likelihood is hard because the number of possible states for $\mathbf{Z}$ is very large, and we use a standard trick for fitting HMMs, to base or inference on the complete data likelihood:


\begin{align}
P(\mathbf{D},\BZ, \BH|\theta) &= P(\mathbf{D}|\mathbf{Z},\BH, \theta) P(\BZ |\theta)P(\BH |\theta) \nonumber\\
&= \prod_w P(D_w|Z_w,H_w, \theta) \prod_w P(H_w |\theta) \prod_w P(Z_w |Z_{w-1},\theta) P(Z_0|\theta)
\end{align}


Here, $P(D_w|Z_w,  H_w, \theta)$ is the emission probability for window $w$,  $P(Z_w|Z_{w-1},\theta)$ is the transition probability, and $P(Z_0| \theta)$ is the initial probability $\pi$. $P(H_w |\theta)$ is the output of ROH HMM described in a following section.

Using the complete data likelihood allows us to factor the emission and transition probabilities, and hence we can maximize them independently.
Transition matrix for different cases of relatedness are obtained from simulations (see section). Initial transition probability $\pi_i$ is same for all the $i$.

\subsection{Emission probability}
In equation \ref{eq:cll}  the term $P(D_w | Z_w, H_w, \theta) = P(D_w | Z_w, H_w, N_w)$ is the emission probability, i.e. the probability of our data , given a particular hidden state. The number of shared sites $N_w$ is the only parameter in $\theta$ that affects the emissions.

Assuming sites are equally distributed and independent, we could use a  binomial likelihood
$$P(D_w|Z_w, N_w, H_w) \sim \text{Binom}[D_w ;N_w, p(Z_w, H_w)] \text{,}$$

where, $p$  is the expected proportion of differences we would expect for a particular IBD and ROH state.

If the two individuals are unrelated in a particular window and neither is in a ROH region (i.e. $Z_w = H_w = 0$), then the expected proportion of pairwise differences depends solely on the effective population size, and we denote this proportion with $p_0$. If the two individuals share one or even both copies of the genome IBD, we would expect the proportion of differences to be reduced to $p_1 = \frac{3}{4} p_0$, and $p_2 = \frac{1}2 p_0$, respectively, since either one or two of the four possible comparisons will be between identical chromosomes.


$p_0$, $p_1$, and $p_2$ are the expected proportion of differences in $i_0$, $i_1$, and $i_2$. $p_0$ is calculated directly from the data by calculating median of differences for all possible pairs of specimens. We calculate $p_1$ as $(3/4)p_1$ and $p_2$ as $p_1/2$. However, we need one more parameter (represented as $p_4$ in the table) in cases where IBD state is $i_1$ and ROH state is $\omega_2$, or IBD state is $i_2$ and ROH state is $\omega_1$ or $\omega_2$. In such cases the expected proportion of differences between the individuals should be 0. However, as mentioned before, we do all our calculations in windows of 10 Mb, and start/end positions of these windows may not coincide with that of ROH tracts. And so, the pairwise differences in this state (called $p_4$ in the table) can lie anywhere between 0 to $p_2$. We found $p_4$ = $p_2$/2 to be a reasonable assumption for expected pairwise proportion of differences in this IBD/ROH state.


Taken together, we can summarize $p$ as follows 
\begin{equation}\label{eq:p}
    p(Z_w, H_w) = \left[\begin{array}
{rrr}
p_0 & p_1 & p_2 \\
p_0 & p_2 & p_4 \\
p_0 & p_4 & p_4
\end{array}\right]
\end{equation}



The effect of these considerations is that even though we have nine possible combinations of $Z$ and $H$ for each window, there are actually only four different $p$-parameters.



However, we find that the data often has considerably higher variance than would be expected from a binomial distribution, likely because we average over a large number of SNPs with different allele frequencies. 

We therefore add an overdispersion parameter $\delta$ that allows for variation in the $p$ between SNPs. This results in the betabinomial likelihood, (we show that such a distribution fits the data well (supplementary fig)).  

\begin{equation}\label{eq:bbsimple}
P(D_{w}|p_i,\delta_i,N_w) = \binom{N_w}{D_w}\frac{B(D_w+p_i \delta_{i}, N_w-D_w+ \delta_{i}(1-p_{i}))}{ B(p_{i}\delta_{i}, (1-p_{i})\delta_{i})}
\end{equation}








\subsubsection{Estimation}

Emission probabilities are estimated with Expectation-Maximization (EM) algorithm. In each iteration of the algorithm, the Expectation step involves forward-backward algorithm to calculate joint probability of the observed data and the IBD states $Z$, given an initial guess of $\delta$. In maximization step we use these joint probabilities to optimize $\delta$, and update emission probabilities. Iterations are run, until the complete data likelihood stops changing. 

\paragraph{Initialization}
The value of $\delta$ is unknown to start with, and is randomly assigned, such that the mean of beta distribution and variance is within constraints described in maximization section.

\paragraph{Expectation step}

In this step, using the standard forward-backward algorithm, we calculate the posterior probability of each IBD state in each window $\gamma_{wi}$. Expectation step, among other parameters, requires emission probabilities. We calculate emission probabilities for each case of $Z_w$ and $H_w$ from $\delta$ and $p$ using equation 3, and we get the total emission probabilities for each $Z_w$ as $\sum_{H_w} P(D_w|Z_w,N_w,H_w) P(H_w|\theta)$

\paragraph{Maximization step}

The only free parameters we estimate in the M-step are the overdispersion parameters $\delta_j$. We do this optimization using a cost function, which is the log-emission probability weighted by the posterior probabilities of the hidden states $\gamma_wi$ and the ROH state-probabilities and $h_{w\omega}$.


\begin{equation}
\mathcal{C} = \sum_{w=1}^L \sum_{i=0}^2\sum^2_{\omega=0} \log P(D_{w}|Z_w, H_w, N_w, \delta, p)h_{w\omega}\gamma_{wi}
\end{equation}


Using equation \ref{eq:p}, we simplify this by grouping all the terms that would result in the same $p$, i.e.

\begin{align*}
k_{w0} &= \gamma_{w0}\\
k_{w1} &= \gamma_{w1} h_{w0}\\
k_{w2} &= \gamma_{w2} h_{w0} + \gamma_{w1} h_{w1}\\
k_{w4} &= \gamma_{w2} h_{w1} + \gamma_{w2} h_{w2} + \gamma_{w1} h_{w2}
\end{align*}

So we can rewrite Eqn 4 as:
\begin{align}
\mathcal{C} &= \sum_{w=0}^L\sum_{j=0}^4  \log P(D_{w}|Z_w, H_w, N_w, \delta_j, p_j)k_{wj}
\end{align}

Since each term in this equation depends on only one $\delta_j$, we can optimize for them independently.


We observed that estimating the $\delta_j$ without constraints can result in wide Beta distributions with very high variance, leading to over fitting of data. For example, a parent  shares exactly one chromosome IBD with its child throughout the genome, so we expect only one IBD state. However,  running an unconstrained model could use this data to fit a pair of siblings with a very wide beta-distribution (with a very high $\delta_1$, which causes problems in differentiating between parent-child and siblings.

We avoid this problem by constraining the parameter space of $\delta$. 
In particular, 
Variance of beta distribution is shown below:
\begin{align}
    X \sim B(p,\delta)\\
    var(X) = \frac{p(1-p)}{\delta + 1}
\end{align}
We find the value of $\delta$ for which the variance is less than a threshold t.

We solve the following equation for $\delta$:
\begin{equation}
    \frac{p(1-p)}{\delta + 1}  < t
\end{equation}

We choose t for each beta distribution to be proportional to the mean ($p$), since the variances of beta distribution corresponding to different $x$ states follow the same relation as the distribution means (supplement figure). 
\begin{align}
    Var(X_{x_1}) = 3/4 Var(X_{x_0})\\
    Var(X_{x_2}) = 1/2 Var(X_{x_0})
\end{align}


Ideally, t for $x_4$ should be zero, but since our genomic windows may not coincide with the IBD fragments or ROH, the variance can be much higher. We find that fixing t for $x_4$ at the same value as $x_0$ works well.



\subsection{Relatedness classes}
This model is run for all the different cases of relatedness using the corresponding transition matrix, and the likelihood for all the relatedness models is compared. The relatedness for the maximum likelihood model is reported, and the confidence is given by the log likelihood ratio between the two highest likelihood models.  
Hidden state of each window (IBD state) is estimated using standard Viterbi algorithm, and the IBD states corresponding to the most likely model are returned. 



\subsection{ROH estimation model}

Our HMM to detect ROH tracts works similar to the relatedness model described above. We sample all possible pairs of reads from each individual at all the positions where there are at least two reads, and calculate the proportion of read pairs that are different. These proportion of differences at each position are summed up in windows along the genome. This forms the input to the HMM, along with the number of overlapping sites, and the output is the posterior probability of a window being homozygous. This probability can also be inferred as the proportion of a window that has ROH. Our model has two hidden states: homozygous ($y_0$), and non homozygous ($y_1$). $Y_w$ denotes the state $y$ in a window $w$, and the vector of $Y_w$'s is denoted with $\mathbf{Y}$. The complete data likelihood for the model in this case is shown below:

\begin{align}
    P(\mathbf{\Delta},\mathbf{Y}|\Theta) &= P(\mathbf{\Delta}|\mathbf{Y},\Theta) P(\mathbf{Y}|\Theta)\nonumber\\
 &= [\prod_{w} P(\Delta_w|Y_w, \Theta) \prod_{w} P(Y_w|Y_w-1, \Theta)] P(Y_0| \Theta)
\end{align}


In this case, transitions are not known, and both transitions and emissions are estimated with standard EM algorithm. The emissions are calculated using betabinomial likelihood, and the mean of the distributions are constrained at expected proportion of differences in a ROH tract ($\rho_{0}$) and expected proportion of differences in a non-ROH tract $\rho_{1}$. Parameters $\rho_{1}$ and $\rho_{0}$ are the same as $p_{2}$ and $p_4$ in previous model. 

\subsection{Emissions}
Similar to our relatedness model, we estimate emissions using EM algorithm with expectation and maximization steps. The expectation step outputs the posterior probability $\Gamma$ of being in $y_0$ and $y_1$ in each window. Maximization step uses the $\Gamma$ to optimize the $\Delta$ and $\rho$ parameters with a betabinomial cost function:

\begin{align}
\mathcal{C} = \sum_{w} \sum_{y} P(\Delta_w|\eta_{wy},\rho_{wy}) \Gamma_{wy} 
\end{align}

In case of very noisy data, we see high pairwise differences in some windows that are not explained with the beta binomial distributions estimated for $y_0$ and $y_1$. To avoid our distributions from flattening to explain these high differences, we force these high difference windows to $y_1$ (see supplement figure). 

\subsection{Transitions}
Transitions are estimated using standard Baum-Welch algorithm. Expectation step is the same as that for emissions, and gives the posterior probability. In maximization step, we use the posterior probability to count how many times transitions are made from state i to j, normalized by the total number of times we are in state i. We observe that the final results are not affected by initial guess of the transition probabilities and assign a probability of 0.8 to stay in the same state for both $y_0$ and $y_1$. 

\subsection{Contamination}

Contamination by present-day people is a common feature in ancient DNA \cite{peyregne_present-day_2020} . Even low levels of contamination may make samples look less similar, and thus distort relatedness analyses. This issue is particularly pronounced  when analyzing populations with  low diversity, where even $<5\%$ contamination can cause loss of power to detect family relations (supplementary figure with READ?, lcMLkin at 4x). To address this issue, we  adjust the  pairwise differences $D_w$, given the contamination estimates of both samples and the average divergence between the target population and a putative contaminating population.

For each pair of individuals $i$ and $j$ we approximate the contamination $C_{i,j}$ as the sum of contamination for each individual $C_i + C_j$. Then, $C_{i,j}$ represents the probability of comparing a contaminant read from an individual to an endogenous read from the other individual. Throughout, we assume that contamination levels are low, so that we can ignore comparison between contaminant reads, i.e. $C_iC_j \approx 0$.  We denote the average divergence between target and contaminating populations as $p_c$, and the observed proportion of differences between individuals $i$ and $j$ as $p_{aij}$. We calculate average endogenous pairwise difference $p_eij$ for individuals i and j as follows \cite{noauthor_ancient_nodate}:
\begin{align}
    p_{aij} = C_{ij}  p_c + (1-C_{ij})  p_{eij}\\
    or, p_{eij} = \frac{p_{aij} - C_{ij}  p_c}{1-C_{ij}} 
\end{align}

Given $p_{eij}$, we would ideally like to calculate the complete data log likelihood for the relatedness model while summing over all possible differences we could have in a window weighted by the probability of seeing that difference. 



\paragraph{Short version}
With contamination, we do not observe the number of differences $D$ and total number of comparisons directly $N$, but instead need to approximate them. The window-subscript is omitted in the following for clarity:
\begin{equation}
    P(D_{\text{obs}} | Z, N_\text{obs}, \theta) \approx 
    P( \hat{D_w} | \hat{N_w}, Z, \theta)
\end{equation}
We do that by calculating the probability that a single read is endogenous given it has a difference.
\begin{align}
    \hat{D} = \mathbb{E}[D | D_{\text{obs}}, c] &= 
    D_{\text{obs}} P(E_i | D_i=1) \\
    &= D_\text{obs} \frac{P(D_i=1 | E_i)P(E_i)}{P(D_i=1)}\\
    &= D_\text{obs} \frac{(1-c)p_e}{(1-c)p_e + c p_c}
\end{align}

and likewise for reads that do not have a difference
\begin{align}
    \hat{S} = \mathbb{E}[S | S_{\text{obs}}, c] &= 
     S_\text{obs} \frac{(1-c)(1-p_e)}{(1-c)(1-p_e) + c (1-p_c)}
\end{align}

and $\hat{N} = \hat{S} + \hat{D}$

\begin{equation}
    P(D_{\text{obs}} | Z, N_\text{obs}, \theta) = \sum_n\sum_d P(D_{\text{obs}}, N_\text{obs} | D_w=d, N_w, C_i, C_j)\underbrace{P(D_w=d | N_w, Z, \theta)}_\text{old likelihood}
\end{equation}

i.e. we sum over all possible $D_w$, $N_w$. Instead, we use

\begin{align}
    P(D_{cor},Z|D,H,\theta,c,p_c) &= P(D_{cor}|Z,D,H,\theta,c,p_c) P(Z|D,H,\theta,c,p_c)\nonumber\\
    &= [\prod_{w} [\sum_\kappa P(D_{cor,w}|Z_w, H_w, \theta,c,p_c) P(D_{cor,w}=\kappa)] \prod_{w} P(Z_w|Z_{w-1}, \theta)] P(Z_0| \theta)
\end{align}



We realize that this calculation would be time consuming, and would further complicate the HMM. To avoid this, instead of using weighted sum of all possible $\kappa$, we use $E(D_{cor,w}|Z_w, H_w, \theta,c,p_c)$. This simplifies the equation 21 to :

\begin{align}
    P(D_{cor},Z|D,H,\theta,c,p_c) &= P(D_{cor}|Z,D,H,\theta,c,p_c) P(Z|D,H,\theta,c,p_c)\nonumber\\
    &= [\prod_{w} P(E(D_{cor,w})|Z_w, H_w, \theta,c,p_c) \prod_{w} P(Z_w|Z_{w-1}, \theta)] P(Z_0| \theta)
\end{align}

We calculate $E(D_{cor,w})$ as shown:

\begin{align}
    E[N_e] + E[N_c] = N\\
    E[D_e] + E[D_c] = D
\end{align}

Probability of comparing endogenous reads at a site where we see a difference:
$$P(\xi | N = 1,D = 1)=\frac{(1-c) p_e}{c p_c + (1-c) p_e} $$
Probability of comparing endogenous reads at a site where we see no difference:
$$P(\xi | N = 1,D = 0)=\frac{(1-c)(1-p_e)}{c (1-p_c) + (1-c) (1-p_e)} $$
Expectation of number of endogenous comparisons in a window:
\begin{align}
    E[\xi | N, D] = D E[\xi | N=1, D=1] + (N-D) E[\xi | N=1, D=0]\\
    = D P(\xi | N=1, D=1) + (N-D) P(\xi | N=1, D=0)
\end{align}



Total number of endogenous sites showing a difference in a window: 
$$D_{cor,w} = D_w* \frac{p_e(1-c)}{p_e(1-c) + c p_c} $$
Total number of endogenous sites showing 0 difference in a window: 
$$S_{cor,w} = S_w* \frac{(1-p_e)(1-c)}{(1-p_e)(1-c) + c (1-p_c)} $$
Total number of endogenous sites in a window:
$$N_{cor,w} = D_{cor,w} + S_{cor,w}$$

This model outputs the contamination corrected number of differences and overlapping sites in each window.

\subsection{Simulations}

\paragraph{ Simulations for estimation of transition matrix}
We use simulations both for estimating the transition matrices for all our models, and for testing and validating our algorithm. All simulations are performed using \textt{msprime} \cite{kelleher_efficient_2016}, followed by simulations of related indivudals using a predetermined pedigree (Figure S1).

For our simulations of background diversity, we simulated a haploid population (pop1) with constant effective size of 10,000 and sampled 16 haploids from 2500 generations ago to randomly combine into 8 unrelated diploid ancient individuals. For each individual, we simulated 22 chromosomes with length $L=100$ Mb. The mutation rate was set as $\mu= 10^{-8}$ per base pair per generation and recombination rate used was $r=10^{-8}$ 

For artificially mating two individuals, we simulated a recombined chromosome from each individual to be passed on to the progeny. For simulating a recombined chromosome from a diploid individual, we drew the number of recombination points from a Poisson distribution with parameter $rL$ as the recombination rate, and used a uniform distribution to sample the positions of recombination points. We estimated transition matrices corresponding to different cases of relatedness by counting transition between IBD states for a pair of  individuals at a particular relatedness level. We realize that we have low power to detect $4^{th}$ and $5{th}$ degree and cna differentiate these from unrelated. In our final output, we show classifications of $4_{th}$ and $5_{th}$ degrees as unrelated.   

\paragraph{Model Comparison}
model with maximum likelihood is reported, and log likelihood ratio is calculated with the two highest likelihood models. 


#### Simulations for model evaluation
Apart from the related and unrelated individuals in pop1, we simulated more haploids in three other populations to create scenarios with ascertainment and contamination. We simulated two haploids to form an individual each from two other populations (pop2, pop3) with split time of 3500 and 4500 generations with pop1, and sampling time of 4000 generations and 2000 generations ago respectively. We identified the sites that were polymorphic among these two individuals, and used these sites to ascertain the genomes of individuals from pop1. We test out the performance of our method in presence of long ROH ($\sim17\%$), by simulating regions of homozygosity with a markov chain using transition matrix shown below: 

$$\mathbf{A} = \left[\begin{array}
{rr}
1-10/l & 10/l \\
2/l & 1-2/l  \\
\end{array}\right]
$$

From the steps described above, we got genotypes of 17 individuals in pop1 in presence/absence of ROH and ascertainment. We further simulated 10 haploids (5 diploid individuals) from another population (pop4) with split time of 20,000 generations with pop1, sampled from the present time. We calculated the allele frequency for pop4 at each site, and used it to introduce contamination (with fixed contamination percentage for each individual ($<5\%$) in samples from pop1. We generated reads (derived/ancestral) for different genomic coverages ranging from 4x to 0.03x at each position for each individual. We specifically generated reads for four different cases: Ancestral-Endogenous, Ancestral-Contaminant, Derived-Endogenous, Derived-Contaminant. The number of reads for each case was sampled from a Poisson distribution with $\lambda$ calculated from the coverage, allele frequency or genotype (G $\in$ 0,0.5,1), and contamination estimate:

\begin{align}
    \lambda_{DE} = \zeta (1-c) G\nonumber\\
    \lambda_{AE} = \zeta (1-c) (1-G)\nonumber\\
    \lambda_{DC} = \zeta c \upsilon\nonumber\\
    \lambda_{AC} = \zeta c (1-\upsilon)
\end{align}


\section{Notation summary}
Notation used in the paper:

\begin{itemize}
\item W: Total number of windows
\item w: Index of window
\item $I$:IBD states base
\item $Z_w$: IBD state in window w
\item $\mathbf{Z}$: Vector of $Z_w$'s
\item $\omega$:ROH states base
\item $H_{w\omega}$: ROH state in $w$
\item $s$: SNP
\item $d_s$: pairwise difference at $s$
\item $\mathbf{R_{si}}$: Vector of reads mapped to snp s for individual $i$
\item $n_{si}$: Number of reads at SNP s for individual $i$
\item $a_{si}$: Number of reads with alternate allele at SNP s for individual $i$
\item $N_{wij}$: Number of SNPs overlapping for individuals $i$,$j$ in window $w$
\item $D_{w}$: Number of SNPs in window w where a pair of individuals differ
\item $\mathbf{D}$: Vector of $D_w$
\item $\pi$: Initial transition probability
\item $A$: Transition matrix
\item $theta$: vector of additional parameters ($\pi$, $A$ and $N_w$)
\item $\psi$: Matrix of parameters representing expected proportion of differences in different $I$ and $\omega$
\item $psi_{I\omega}$: Element on row $I$ and column $\omega$ of $\psi$ matrix
\item $x$: All combinations of $I$ and $\omega$ states with unique $\psi_{I\omega}$
\item $p$: Proportion of differences in $x$
\item $\alpha,\beta$: Parameters of Beta distribution
\item $\delta$: $\alpha + \beta$ for relatedness model
\item $\gamma_{wI}$: posterior probability of $I$ in $w$
\item $k_{wI}$: Posterior probability of $x$ in $w$ 
\item $\nu_{w}$: Allele frequency in window w
\item X: Random variable
\item r: Relatedness between individuals
\item $\theta$: Vector of initial probability of being in an IBD state $I_l$
\item $/mathbf{B}$: Beta function
\item $B$: Reparameterized Beta function
\item $Delta$: Heterozygosity in an individual
\item Y: Hidden Homozygosity state
\item $\Theta$: vector of additional parameters ($\pi$ $N_w$)
\item $Gamma_{wY}$: Posterior probability of Y in w
\item $/eta$: $/alpha + /beta$ for ROH model
\item $c$: Contamination estimate for a library
\item $p_c$: Average proportion of differences between endogenous and contaminating reads
\item $p_e$:Average proportion of differences between endogenous reads
\item $p_a$: Average observed proportion of differences
\item $D_{cor,w}$ Contamination corrected number of differences in $w$
\item $N_{cor,w}$ Contamination corrected number of overlapping sites in $w$
\item $\Xi$: Endogenous read
\item $S_{cor,w}$ $N_{cor,w}$ - $D_{cor,w}$
\item $/mathbf{D_cor}$ Vector of $D_{cor,w}$ 
\item E: Expectation
\item P: Probability
\end{itemize}

\bibliographystyle{plain}
\bibliography{KIn_ref.bib}


\section{Results}
We predict the following categories of relatedness with KIn: Identical individuals (twins), First degree (parent-child), First degree (siblings), Second degree, Third degree, Unrelated ( including higher than third degree relatives). We apply KIn ROH model on simulated data with different coverages:


\section{Discussion}

We show that READ outputs higher false positives in case of recent inbreeding in the population, and higher false negatives in case of contamination. 


\section{extra}
Many methods exist to estimate relatedness. Most of these work with high coverage, phased data, reference panel, population allele frequency. Other methods that work with low coverage ancient data use likelihoods, but still require 2x coverage.
methods that require a lot of things: PLINK, SNPduo,ERSA,KING,REAP,GRAB,

methods that require low coverage: SEEKIN

mtDNA and Y in relatedness READ paper[15,16,25,16]
snp data better than str when there is damage, low data
excluding related individuals, haak indo europian

READ is a very popular software to estimate relatedness in low coverage ancient DNA. It calculates distance between a pair of individuals using pseudo haploid sequences. These pairwise differences are then normalized by the median of pairwise differences in the population, and compared to the expected pairwise difference given a degree of relatedness. Expected pairwise difference for each case of relatedness is calculated using identity by descent (IBD) probabilities. 

lcMLkin is another widely used method that uses genotype likelihoods to estimate the number of positions at which a pair of individuals have 0/1/2 chromosomes in IBD.  

REAP is a software to estimate relatedness coefficient and IBD prbabilities in admixed populations, accounting for structure. It requires the number of ancestral populations, and a representation of sub population allele frequencies. 

Likelihood estimators:
Milligan BG (2003) Maximum-likelihood estimation of relatedness. Genetics
Anderson AD, Weir BS (2007) A maximum-likelihood method for the estimation of pairwise relatedness in structured populations. Genetics
Choi Y, Wijsman EM, Weir BS (2009) Case-control association testing in the presence of unknown relationships

There are various ways to measure the coefficient of relatedness r. Most methods use the probabilties of IBD to calculate r. There can be, in total 9 modes of identity described by Jacquard (1974), including the cases for 3 IBD probabilities and 3 cases of IBD. Methods assuming outbred population use only the three modes accounting for IBD. Most methods used maximum likelihood approach to estimate r (). Other methods are moment estimators that assume Hardy-Weinberg equilibrium in the population (KING, GCTA).

#Methods
$$\sum_{w} \sum_{I}\sum_\omega \log P(D_{w}|Z_w, H_w)k_{w\omega i}$$



$$P(D_w | N_w, Z_w) = \sum_{k=0}^4 
\underbrace{P(D_w | N_w, k)}_{Betabinomial}
\underbrace{P(k | Z_w, H_w)}_{p matrix}
\underbrace{P(H_w =h)}_{\mathbf{h}_w}
$$

Emission probability in a window depends on both IBD state and ROH state. We obtain posterior probability of one or both individuals being ROH in each window from another HMM described in section 3.2. We calculate the cost function with Betabinomial likelihood in log space for the entire genome as:

The model can be summarised as:

\begin{equation}\label{eq:6}
P(D_w|Z_w,H_w) \sim {\sf Binom}(D_w, N_w, p_{Z,H,w})
\end{equation}

\begin{equation}\label{eq:7}
p_{Z,H,w} \sim {\sf Beta}(\alpha_{Z,H}, \beta_{Z,H})
\end{equation}

Emission probability in a window depends on both IBD state and ROH state. We calculate a cost function as the betabinomial loglikelihood weighted by the posterior probability represented by \gamma.
likelihood:
\begin{align}
P(\mathbf{D},\BZ, H|\theta) &= P(\mathbf{D}|\mathbf{Z},H,\theta) P(H |\theta) P(\BZ|\theta)\nonumber\\
&= \prod_{w}  P(D_w|Z_w,  H_w, \theta) \prod_w P(H_w | \theta) \prod_{w} P(Z_w|Z_{w-1}, \theta)] P(Z_0| \theta) 
\end{align}

cost function:
\begin{equation}
    \mathcal{C} = 
       \sum_{w} \sum_{i} \log P(D_{w}|Z_w, H_w)\gamma_{wi}=
    \sum_{w} \sum_{i} 
    \log \left[\sum_{\omega}P(D_{w}|\psi_{i\omega},\delta_{i\omega}) \right] \gamma_{wi}
\end{equation}

\end{document}



