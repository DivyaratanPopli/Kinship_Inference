# 1) enter a cruncher, chose a free one
ssh bionc{nr}
# see the cruncher's business
htop


# 2) enter my conda environment
# if first time, do
# conda env create -f /home/luca_traverso/scripts/env_KIN_lt.yml 
# (does it work?)
conda activate snakemake_lt


# 3) decide how many cores to employ
# Divy's suggestion: the number of available ones -4/5
ncores=40


# 4) if you are collaborator for the Divy's directory
git clone https://github.com/DivyaratanPopli/KIn_snakemake.git
# git@github.com:DivyaratanPopli/KIn_snakemake.git if you have a github auth
cd KIn_snakemake/


# 4.1) If you are not
#cp -r /home/luca_traverso/Pedigrees/KIN_lastversion/KIn_snakemake
#cd KIN_snakemake/

 
# 5) create a list with the absolute paths of the samples, one per line
#inside the KIN_snakemake directory
list=list_samples.txt


# 6) run the script to prepare the run
bash /home/luca_traverso/scripts/prepare_4_kin.sh $list


# 7) run the first part of the pipeline
snakemake all --cores $ncores --config is_contam=0


# 8) once it finished, run the model
conda deactivate
cd hmm_numba/ 
bash looprel.sh n=$ncores



###########################################################################################################################
# RESULTS 

# model's pairwise reults are in {path_to_dir}/hmm_numba/filtered0/merged_relatable_allLikelihoods.csv
# filtered overlaps are in {path_to}/overlap_fil1
# unfiltered overlaps are in {path_to}/overlap_fil0
# likelihoods for each pair (all models) are in {path_to}/hmm_numba/filtered0/allLikelihoods/pw_{sample1}_{sample2}.csv
# to see the likelihood for a specific column use:
# awk -F "," '{print $X}' {couple}.csv
# Xs: 2 un ; 3 deg5 ; 4 deg4 ; 5 deg3 ; 6 gr ; 7 hsib ; 8 avu ; 9 sib ; 10 pc ; 11 id

# READ results are in  {path_to}/hmm_numba/filtered0/read/READ_results

